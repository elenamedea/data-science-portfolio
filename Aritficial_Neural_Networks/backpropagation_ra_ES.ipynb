{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (ANN) from scratch - with Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Agenda<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Artificial-Neural-Network-(ANN)-from-scratch---with-Backpropagation\" data-toc-modified-id=\"Artificial-Neural-Network-(ANN)-from-scratch---with-Backpropagation-1\">Artificial Neural Network (ANN) from scratch - with Backpropagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recap-of-Feed-Forward-Networks:\" data-toc-modified-id=\"Recap-of-Feed-Forward-Networks:-1.1\">Recap of Feed Forward Networks:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Start-with-making-some-random-data-with-make_moons-from-Scikit-Learn\" data-toc-modified-id=\"Start-with-making-some-random-data-with-make_moons-from-Scikit-Learn-1.1.1\">Start with making some random data with <code>make_moons</code> from Scikit Learn</a></span></li><li><span><a href=\"#Building-the-network\" data-toc-modified-id=\"Building-the-network-1.1.2\">Building the network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Activation-function\" data-toc-modified-id=\"Activation-function-1.1.2.1\">Activation function</a></span></li><li><span><a href=\"#Adding-the-bias\" data-toc-modified-id=\"Adding-the-bias-1.1.2.2\">Adding the bias</a></span></li><li><span><a href=\"#Generating-the-weights-of-the-network\" data-toc-modified-id=\"Generating-the-weights-of-the-network-1.1.2.3\">Generating the weights of the network</a></span></li></ul></li><li><span><a href=\"#Layer-1\" data-toc-modified-id=\"Layer-1-1.1.3\">Layer 1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weights-for-L1\" data-toc-modified-id=\"Weights-for-L1-1.1.3.1\">Weights for L1</a></span></li><li><span><a href=\"#Activation-in-L1\" data-toc-modified-id=\"Activation-in-L1-1.1.3.2\">Activation in L1</a></span></li></ul></li><li><span><a href=\"#Layer-2\" data-toc-modified-id=\"Layer-2-1.1.4\">Layer 2</a></span><ul class=\"toc-item\"><li><span><a href=\"#Weights-for-L2\" data-toc-modified-id=\"Weights-for-L2-1.1.4.1\">Weights for L2</a></span></li><li><span><a href=\"#Activation-in-L2\" data-toc-modified-id=\"Activation-in-L2-1.1.4.2\">Activation in L2</a></span></li></ul></li><li><span><a href=\"#Implement-the-Feed-Forward-Function\" data-toc-modified-id=\"Implement-the-Feed-Forward-Function-1.1.5\">Implement the Feed-Forward Function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Loss-Function-and-Calculate-Loss\" data-toc-modified-id=\"Define-Loss-Function-and-Calculate-Loss-1.1.5.1\">Define Loss Function and Calculate Loss</a></span></li></ul></li></ul></li><li><span><a href=\"#Next-Step:-Backpropagation\" data-toc-modified-id=\"Next-Step:-Backpropagation-1.2\">Next Step: Backpropagation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Main-take-away-messages\" data-toc-modified-id=\"Main-take-away-messages-1.2.0.1\">Main take-away messages</a></span></li></ul></li><li><span><a href=\"#Small-aside:-The-Chain-Rule\" data-toc-modified-id=\"Small-aside:-The-Chain-Rule-1.2.1\">Small aside: The Chain Rule</a></span></li><li><span><a href=\"#Steps-of-Backpropagation-in-our-Network:\" data-toc-modified-id=\"Steps-of-Backpropagation-in-our-Network:-1.2.2\">Steps of Backpropagation in our Network:</a></span></li><li><span><a href=\"#How-does-Log-Loss-work?\" data-toc-modified-id=\"How-does-Log-Loss-work?-1.2.3\">How does Log Loss work?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Worked-example\" data-toc-modified-id=\"Worked-example-1.2.3.1\">Worked example</a></span></li></ul></li><li><span><a href=\"#Equation-A.-Take-The-Derivative-of-The-Loss-Function\" data-toc-modified-id=\"Equation-A.-Take-The-Derivative-of-The-Loss-Function-1.2.4\">Equation A. Take The Derivative of The Loss Function</a></span></li><li><span><a href=\"#Equation-B.-Calculate-The-Derivative-of-The-Activation-Function\" data-toc-modified-id=\"Equation-B.-Calculate-The-Derivative-of-The-Activation-Function-1.2.5\">Equation B. Calculate The Derivative of The Activation Function</a></span></li><li><span><a href=\"#Equation-C.-Calculate-Weight-Change-between-2-Layers\" data-toc-modified-id=\"Equation-C.-Calculate-Weight-Change-between-2-Layers-1.2.6\">Equation C. Calculate Weight Change between 2 Layers</a></span></li><li><span><a href=\"#Equation-D.\" data-toc-modified-id=\"Equation-D.-1.2.7\">Equation D.</a></span></li><li><span><a href=\"#Equation-E.-Calculate-Change-in-the-first-Set-of-Weights.\" data-toc-modified-id=\"Equation-E.-Calculate-Change-in-the-first-Set-of-Weights.-1.2.8\">Equation E. Calculate Change in the first Set of Weights.</a></span></li></ul></li><li><span><a href=\"#Next-steps:\" data-toc-modified-id=\"Next-steps:-1.3\">Next steps:</a></span></li><li><span><a href=\"#Some-cool-links\" data-toc-modified-id=\"Some-cool-links-1.4\">Some cool links</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap of Feed Forward Networks:\n",
    "NB: This section is essentially a repeat from yesterday's notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with making some random data with `make_moons` from Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_moons(n_samples=200, noise=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10689665,  0.04229281],\n",
       "       [ 0.95679964,  0.45675049],\n",
       "       [ 0.73351628,  0.58461744]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First three datapoints in X:\n",
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f925985b940>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the network\n"
   ]
  },
  {
   "attachments": {
    "ffn.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ffn.png](attachment:ffn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two neurons in the hidden layer\n",
    "- One neuron in the output layer\n",
    "- Sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + e** -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also immediately define the derivative of the logistic function as we will need it for backpropagation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(x):\n",
    "    \"\"\" This function returns the derivative of the sigmoid.\"\"\"\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus: you could also try it with other activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also try this with other activation functions e.g. relu:\n",
    "def relu(x):\n",
    "    return max(0, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or tanh:\n",
    "def tanh(x):\n",
    "    return (e**x - e**-x) / (e**x + e**-x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X, np.ones((X.shape[0],1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.10689665,  0.04229281,  1.        ],\n",
       "       [ 0.95679964,  0.45675049,  1.        ],\n",
       "       [ 0.73351628,  0.58461744,  1.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the weights of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise our weights and biases randomly (there are multiple other ways of doing this - more on this tomorrow!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What shape do our weights need to be?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first layer, we have:\n",
    "- 2 inputs and one bias, so three weights to be learned per neuron in hidden layer\n",
    "- 2 neurons in the hidden layer\n",
    "\n",
    "So the shape is: `(3,2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can choose random weights from the standard normal distribution, mean 0 s.d. 1:\n",
    "weights1 = np.random.normal(0,1, (3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going into the output layer we have:\n",
    "- One weight per output of each neuron plus the bias for each neuron of output layer\n",
    "- One neuron in the output layer\n",
    "\n",
    "So the shape is `(3,1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = np.random.normal(0,1,(3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights for L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_1 = np.dot(X, weights1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just multiplied a $200 \\times 3$ matrix with a $3 \\times 2$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights in the first layer:\n",
      " [[ 0.56415322 -0.23374001]\n",
      " [ 0.35640848  1.39480421]\n",
      " [-0.1165223   0.41516355]]\n",
      "\n",
      "weights in the second layer:\n",
      " [[-0.31345624]\n",
      " [ 0.16260876]\n",
      " [ 0.56998151]] \n",
      "\n",
      "input to layer 1:\n",
      " [[-7.25908095e-01  7.32879769e-01]\n",
      " [ 5.86049051e-01  8.28598700e-01]\n",
      " [ 5.05655885e-01  1.05913831e+00]\n",
      " [ 4.00274826e-01 -2.75909608e-01]\n",
      " [ 1.66590003e-03  3.67208714e-01]\n",
      " [ 2.18877525e-01  7.70159592e-01]\n",
      " [-2.47323565e-01  1.74953890e+00]\n",
      " [ 6.94005845e-01 -9.20683320e-02]\n",
      " [ 1.02239251e+00 -1.15289063e-01]\n",
      " [ 2.91521333e-01  1.66010736e+00]\n",
      " [-1.03871737e-01  1.36849209e+00]\n",
      " [ 9.22104713e-01 -3.18434382e-01]\n",
      " [ 6.36655927e-01 -5.34922476e-01]\n",
      " [-4.46588526e-01  1.23223367e+00]\n",
      " [ 9.99155194e-01  2.77288170e-01]\n",
      " [ 9.84212234e-01 -9.87324282e-02]\n",
      " [ 4.51672871e-01  2.47284083e+00]\n",
      " [ 2.63969736e-01 -5.72374926e-01]\n",
      " [-4.95081174e-01  1.73167332e+00]\n",
      " [-4.74829996e-02  1.40326474e+00]\n",
      " [ 1.28031949e-01  3.96107753e-01]\n",
      " [ 6.35683746e-01  1.11487253e+00]\n",
      " [ 1.75973926e-01 -8.50904885e-01]\n",
      " [-3.90175467e-01  1.22924570e+00]\n",
      " [-5.39620487e-02  1.45440324e+00]\n",
      " [-1.29567326e-01  1.52947451e+00]\n",
      " [-2.77355865e-01  1.57989234e+00]\n",
      " [ 3.82913157e-03  1.96612655e+00]\n",
      " [-1.11413622e-01  1.29969793e-01]\n",
      " [-1.84027404e-01 -3.34399149e-01]\n",
      " [ 6.07253227e-01  4.75939690e-01]\n",
      " [ 7.25564613e-01  1.06615061e+00]\n",
      " [ 4.51717730e-01  1.72769418e+00]\n",
      " [-4.40016121e-01  1.60296847e+00]\n",
      " [ 2.39419756e-01 -2.01897495e-01]\n",
      " [ 1.60537063e-02  8.93857537e-01]\n",
      " [ 9.47405877e-02 -5.47850702e-02]\n",
      " [ 8.34802480e-02 -7.94555238e-02]\n",
      " [ 6.50601024e-01 -6.19028878e-01]\n",
      " [-4.40374987e-01  1.41956747e+00]\n",
      " [-2.72786561e-01  1.42297969e+00]\n",
      " [ 1.36300467e+00  1.01942806e+00]\n",
      " [ 3.86150696e-01  1.39823998e+00]\n",
      " [ 6.60695468e-01  1.17341829e+00]\n",
      " [ 1.39012985e-01  4.17960497e-01]\n",
      " [ 1.13063717e+00  9.00751310e-01]\n",
      " [ 5.67828792e-01  1.17276198e+00]\n",
      " [ 6.02947820e-01  1.07109329e+00]\n",
      " [ 2.26291082e-01 -1.96429519e-01]\n",
      " [-3.27754142e-01  1.12335900e+00]\n",
      " [ 1.83868653e-01  7.03849737e-01]\n",
      " [ 7.85572322e-01 -2.66092560e-01]\n",
      " [ 2.59835561e-01  3.99651002e-02]\n",
      " [-6.11552555e-02 -2.33676304e-01]\n",
      " [-8.38617587e-02  2.15594249e-01]\n",
      " [-4.13818795e-01  1.78165256e+00]\n",
      " [ 7.69361325e-02 -3.41923015e-01]\n",
      " [-7.93721065e-01  6.41390819e-01]\n",
      " [ 9.11143432e-01 -3.65448153e-01]\n",
      " [-6.72823054e-01  4.00214668e-01]\n",
      " [ 1.06371249e+00  9.62676852e-03]\n",
      " [-4.44021724e-01  9.04352281e-01]\n",
      " [ 5.25928858e-01  1.56957563e+00]\n",
      " [ 6.13126339e-01  4.92224002e-01]\n",
      " [ 2.84207426e-01  1.11626016e+00]\n",
      " [-9.61401603e-03  7.79933458e-01]\n",
      " [ 3.02473767e-01  1.64250266e+00]\n",
      " [ 9.71541044e-01 -2.43072810e-01]\n",
      " [-3.60471825e-01  1.34148042e+00]\n",
      " [ 2.27558570e-01  1.50190632e-01]\n",
      " [ 1.94354905e-01  2.39586096e+00]\n",
      " [-1.86647057e-01 -3.45116675e-01]\n",
      " [ 6.21712772e-01  3.53315606e-02]\n",
      " [ 3.08660896e-01  1.43800138e+00]\n",
      " [-1.95051917e-01  3.52584191e-01]\n",
      " [-3.51658166e-01  2.64294822e-01]\n",
      " [ 7.96505492e-01  1.57818381e+00]\n",
      " [ 5.68302154e-02 -4.86715831e-01]\n",
      " [ 2.35652741e-01  1.76482968e+00]\n",
      " [ 8.43469928e-02  8.56267800e-01]\n",
      " [ 4.50597325e-01  2.96810338e-01]\n",
      " [-2.37585747e-01  1.49570360e+00]\n",
      " [ 1.77416734e-01  2.22138561e+00]\n",
      " [ 1.09820565e+00  7.61163775e-01]\n",
      " [ 1.16495187e+00  2.89964767e-01]\n",
      " [-4.30893012e-01  1.17488903e+00]\n",
      " [ 4.54148892e-01 -4.26894765e-01]\n",
      " [ 5.08777553e-01  1.41586583e+00]\n",
      " [ 9.71177091e-01  1.91512926e-01]\n",
      " [ 2.26033678e-01  2.16223552e+00]\n",
      " [ 6.27022418e-01  1.27791163e+00]\n",
      " [ 1.23428591e-01 -1.29242391e-01]\n",
      " [ 6.22915822e-02  8.44303223e-01]\n",
      " [ 1.85504663e-01  1.24930823e+00]\n",
      " [ 5.42825489e-01  1.02798982e+00]\n",
      " [ 1.27046955e+00  2.32439291e-01]\n",
      " [ 4.25632079e-01 -4.30426376e-01]\n",
      " [ 5.72615748e-01  7.08742289e-01]\n",
      " [ 5.30899923e-01  1.60545080e+00]\n",
      " [-5.89718172e-03  4.89467831e-01]\n",
      " [ 4.13926732e-01  2.01516900e-01]\n",
      " [ 7.75462221e-01  1.68489008e+00]\n",
      " [-8.75752580e-02 -4.62412713e-01]\n",
      " [ 2.43419278e-01 -8.58665946e-01]\n",
      " [ 1.18627262e-01  8.60190643e-01]\n",
      " [ 9.73634997e-02 -4.23975732e-01]\n",
      " [ 4.21056062e-01  1.21990470e+00]\n",
      " [ 1.66767958e-02  2.03065333e+00]\n",
      " [ 1.35681912e-01  9.27525726e-01]\n",
      " [ 1.74731909e-01 -4.04362478e-01]\n",
      " [ 7.24533875e-01 -2.28712538e-01]\n",
      " [ 5.43848890e-01 -5.71102760e-03]\n",
      " [ 1.20040209e+00  1.01141818e+00]\n",
      " [ 2.16222410e-01 -8.49944786e-05]\n",
      " [-2.08999372e-01  1.67951093e+00]\n",
      " [-1.12674913e-01  1.41465055e-01]\n",
      " [ 1.50667644e-01  1.92431301e+00]\n",
      " [ 3.94360866e-01  1.74994900e+00]\n",
      " [ 3.12711712e-01 -6.54933765e-01]\n",
      " [ 5.09731539e-01  1.49406830e+00]\n",
      " [ 4.69286354e-01  1.94876611e+00]\n",
      " [ 4.75663982e-02  7.47792261e-01]\n",
      " [ 6.58537651e-01 -5.28442835e-01]\n",
      " [ 4.76369301e-01  4.76675737e-03]\n",
      " [ 7.98124292e-01  1.69577017e+00]\n",
      " [ 5.98639616e-01  1.44851550e+00]\n",
      " [ 8.55918095e-01  3.18557855e-01]\n",
      " [ 1.11451627e-01 -8.63702534e-01]\n",
      " [-4.10949478e-01  8.12658479e-01]\n",
      " [ 2.14826480e-01  1.84733774e+00]\n",
      " [ 7.75242089e-01 -1.67890785e-01]\n",
      " [ 4.72449211e-01  2.16430154e+00]\n",
      " [-5.47697756e-01  7.96707210e-01]\n",
      " [ 3.24918990e-01 -6.10333627e-01]\n",
      " [ 3.02991780e-01  1.41260029e+00]\n",
      " [ 1.23865205e-01 -3.80752550e-01]\n",
      " [ 5.29923118e-01  3.79933922e-01]\n",
      " [-3.28481051e-01  7.52461393e-01]\n",
      " [ 1.58948726e-01  1.81688098e+00]\n",
      " [ 8.59187186e-02 -1.40022827e-01]\n",
      " [ 1.08176017e+00 -8.83552077e-02]\n",
      " [ 6.99665547e-01 -5.98576764e-01]\n",
      " [ 8.65793389e-01  2.81655935e-01]\n",
      " [ 4.78314589e-01  2.18649206e+00]\n",
      " [-4.94168833e-01  1.35256327e+00]\n",
      " [ 6.37353136e-01  2.09404527e+00]\n",
      " [ 6.35826642e-01  9.21828687e-01]\n",
      " [ 8.00215408e-02  9.79434065e-01]\n",
      " [-4.45413695e-01  9.90055683e-01]\n",
      " [-5.70534607e-01  1.27020043e+00]\n",
      " [ 1.03921917e+00 -5.01157203e-02]\n",
      " [ 8.34412698e-01 -1.47115470e-01]\n",
      " [-6.33637223e-01  1.55628756e+00]\n",
      " [ 6.09264336e-01  4.59556435e-01]\n",
      " [ 4.11036581e-01  1.45309505e-01]\n",
      " [ 9.65096088e-02  2.21147839e+00]\n",
      " [ 8.39259822e-01 -3.04764968e-01]\n",
      " [ 7.17128793e-02  6.98809657e-02]\n",
      " [ 4.00179771e-01  4.79867862e-01]\n",
      " [-1.88572958e-01  1.73509769e+00]\n",
      " [ 8.12123612e-01 -7.07385185e-01]\n",
      " [ 2.60085296e-02 -2.55501484e-01]\n",
      " [ 5.30928979e-01 -2.42085573e-01]\n",
      " [ 1.00914044e+00 -1.19606902e-01]\n",
      " [ 1.82433440e-01  1.59586078e+00]\n",
      " [ 7.16648379e-02  1.71722321e+00]\n",
      " [-1.34838806e-01  6.81709282e-01]\n",
      " [ 5.54285286e-02  1.82711133e+00]\n",
      " [ 9.97317873e-01 -1.25939839e-01]\n",
      " [ 6.81675187e-02  1.83833993e+00]\n",
      " [ 4.10255820e-01  2.10925612e+00]\n",
      " [-4.72915766e-01  8.99002147e-01]\n",
      " [ 5.05467389e-01  1.28972708e+00]\n",
      " [-5.24522876e-02  1.50876196e+00]\n",
      " [ 7.19272296e-01 -7.13380627e-01]\n",
      " [ 8.70656605e-01 -2.38691072e-01]\n",
      " [ 2.62047457e-02 -4.04947868e-01]\n",
      " [ 5.02444818e-01  1.88023286e+00]\n",
      " [ 3.08441402e-01 -4.64143413e-01]\n",
      " [ 1.10915147e+00 -3.71229052e-01]\n",
      " [ 3.02727826e-01  1.13532370e-02]\n",
      " [ 1.69560838e-01  1.81951033e+00]\n",
      " [ 9.41329249e-02 -6.45027118e-01]\n",
      " [-1.03975910e-01 -1.83475556e-01]\n",
      " [ 3.86202633e-01 -2.81710264e-01]\n",
      " [ 1.96023527e-01  1.11192644e+00]\n",
      " [-7.79521521e-01  6.88840666e-01]\n",
      " [ 2.99780311e-01  5.76949690e-01]\n",
      " [-4.44732457e-01  1.61247604e+00]\n",
      " [-3.49682293e-01  1.40487861e+00]\n",
      " [ 2.61909861e-02 -2.30241601e-01]\n",
      " [ 1.04115199e+00  9.97132451e-01]\n",
      " [ 5.61294139e-01  1.08816563e+00]\n",
      " [ 9.76287122e-01  1.32496233e-01]\n",
      " [-2.91936109e-01  1.49190949e+00]\n",
      " [-6.81365042e-02  7.46890067e-01]\n",
      " [ 4.13698944e-01  6.90247020e-01]\n",
      " [-8.21879125e-01  1.25843794e+00]\n",
      " [ 9.06793644e-01 -1.78318506e-01]\n",
      " [ 8.64914667e-02  1.08885405e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"weights in the first layer:\\n {weights1}\\n\n",
    "weights in the second layer:\\n {weights2} \\n\n",
    "input to layer 1:\\n {input_layer_1}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation in L1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pass our weighted sum that is the input to the first layer through an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_1 = sigmoid(input_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32609331, 0.6754369 ],\n",
       "       [0.6424581 , 0.69605855],\n",
       "       [0.62378756, 0.74252584],\n",
       "       [0.59875369, 0.43145687],\n",
       "       [0.50041647, 0.59078433],\n",
       "       [0.55450197, 0.68355542],\n",
       "       [0.43848237, 0.85189463],\n",
       "       [0.66685745, 0.47699916],\n",
       "       [0.73543837, 0.47120962],\n",
       "       [0.57236854, 0.84025241],\n",
       "       [0.47405539, 0.79713642],\n",
       "       [0.71547076, 0.42105735],\n",
       "       [0.65399714, 0.36936953],\n",
       "       [0.39017218, 0.77420928],\n",
       "       [0.73089245, 0.56888126],\n",
       "       [0.72794322, 0.47533692],\n",
       "       [0.6110369 , 0.92221579],\n",
       "       [0.56561189, 0.360689  ],\n",
       "       [0.37869731, 0.84962633],\n",
       "       [0.48813148, 0.80270144],\n",
       "       [0.53196434, 0.59775215],\n",
       "       [0.65377711, 0.75303639],\n",
       "       [0.5438803 , 0.29924307],\n",
       "       [0.40367506, 0.77368653],\n",
       "       [0.48651276, 0.81067517],\n",
       "       [0.46765341, 0.82192942],\n",
       "       [0.43110214, 0.82918927],\n",
       "       [0.50095728, 0.87719446],\n",
       "       [0.47217537, 0.53244679],\n",
       "       [0.45412255, 0.41717063],\n",
       "       [0.64731397, 0.61678863],\n",
       "       [0.6738312 , 0.74386418],\n",
       "       [0.61104756, 0.84911724],\n",
       "       [0.39173713, 0.83243286],\n",
       "       [0.55957065, 0.44969639],\n",
       "       [0.50401334, 0.70968559],\n",
       "       [0.52366745, 0.48630716],\n",
       "       [0.52085795, 0.48014656],\n",
       "       [0.65714589, 0.35000235],\n",
       "       [0.39165162, 0.8052706 ],\n",
       "       [0.43222313, 0.80580511],\n",
       "       [0.7962476 , 0.73486118],\n",
       "       [0.59535572, 0.80190445],\n",
       "       [0.6594166 , 0.76376233],\n",
       "       [0.53469739, 0.60299511],\n",
       "       [0.75595647, 0.71110387],\n",
       "       [0.63826203, 0.76364389],\n",
       "       [0.64633043, 0.74480477],\n",
       "       [0.55633259, 0.45104991],\n",
       "       [0.41878718, 0.75461124],\n",
       "       [0.5458381 , 0.66904076],\n",
       "       [0.68687983, 0.43386662],\n",
       "       [0.56459587, 0.50998995],\n",
       "       [0.48471595, 0.44184531],\n",
       "       [0.47904684, 0.55369076],\n",
       "       [0.3979968 , 0.8559008 ],\n",
       "       [0.51922455, 0.41534243],\n",
       "       [0.31137024, 0.65506779],\n",
       "       [0.71323409, 0.40964136],\n",
       "       [0.337865  , 0.59873924],\n",
       "       [0.74339937, 0.50240667],\n",
       "       [0.39078309, 0.71184308],\n",
       "       [0.62853308, 0.8277231 ],\n",
       "       [0.64865363, 0.62063021],\n",
       "       [0.57057743, 0.75329436],\n",
       "       [0.49759651, 0.68566577],\n",
       "       [0.57504714, 0.83787519],\n",
       "       [0.72542655, 0.43952924],\n",
       "       [0.41084536, 0.79273329],\n",
       "       [0.55664541, 0.53747724],\n",
       "       [0.54843635, 0.91651114],\n",
       "       [0.45347323, 0.41456711],\n",
       "       [0.65060799, 0.50883197],\n",
       "       [0.57655837, 0.80814496],\n",
       "       [0.45139103, 0.5872441 ],\n",
       "       [0.41298038, 0.56569176],\n",
       "       [0.68922648, 0.82894715],\n",
       "       [0.51420373, 0.38066754],\n",
       "       [0.55864206, 0.85381351],\n",
       "       [0.52107426, 0.7018803 ],\n",
       "       [0.61078124, 0.57366259],\n",
       "       [0.44088139, 0.81693281],\n",
       "       [0.5442382 , 0.90215357],\n",
       "       [0.74992375, 0.68160635],\n",
       "       [0.76223133, 0.57198751],\n",
       "       [0.39391311, 0.76402759],\n",
       "       [0.61162522, 0.39486808],\n",
       "       [0.62451986, 0.80468949],\n",
       "       [0.72535405, 0.54773243],\n",
       "       [0.55626905, 0.89680662],\n",
       "       [0.651814  , 0.78209408],\n",
       "       [0.53081803, 0.4677343 ],\n",
       "       [0.51556786, 0.69937075],\n",
       "       [0.54624363, 0.77718009],\n",
       "       [0.63246945, 0.736526  ],\n",
       "       [0.78082312, 0.5578496 ],\n",
       "       [0.60483017, 0.39402452],\n",
       "       [0.63936653, 0.67012319],\n",
       "       [0.62969298, 0.83277883],\n",
       "       [0.49852571, 0.61998106],\n",
       "       [0.60202906, 0.55020943],\n",
       "       [0.6847013 , 0.84355097],\n",
       "       [0.47812017, 0.38641362],\n",
       "       [0.5605561 , 0.29761814],\n",
       "       [0.52962209, 0.70270048],\n",
       "       [0.52432166, 0.39556578],\n",
       "       [0.60373593, 0.77204678],\n",
       "       [0.5041691 , 0.8839781 ],\n",
       "       [0.53386854, 0.71657304],\n",
       "       [0.54357217, 0.40026466],\n",
       "       [0.67360462, 0.44306981],\n",
       "       [0.63270731, 0.49857225],\n",
       "       [0.76859631, 0.7332976 ],\n",
       "       [0.55384598, 0.49997875],\n",
       "       [0.44793952, 0.84283976],\n",
       "       [0.47186104, 0.5353074 ],\n",
       "       [0.53759582, 0.87261862],\n",
       "       [0.59733205, 0.85194637],\n",
       "       [0.57754702, 0.34187859],\n",
       "       [0.62474354, 0.81668812],\n",
       "       [0.61521483, 0.87531204],\n",
       "       [0.51188936, 0.67869745],\n",
       "       [0.65893182, 0.37088015],\n",
       "       [0.61689017, 0.50119169],\n",
       "       [0.68957311, 0.84498149],\n",
       "       [0.64534501, 0.80976986],\n",
       "       [0.70180712, 0.57897275],\n",
       "       [0.5278341 , 0.29656636],\n",
       "       [0.39868448, 0.69267572],\n",
       "       [0.55350102, 0.86381422],\n",
       "       [0.68465377, 0.45812562],\n",
       "       [0.61596329, 0.89699766],\n",
       "       [0.36639871, 0.68926968],\n",
       "       [0.58052258, 0.3519831 ],\n",
       "       [0.57517372, 0.80417575],\n",
       "       [0.53092677, 0.4059454 ],\n",
       "       [0.62946518, 0.59385717],\n",
       "       [0.41861025, 0.67971479],\n",
       "       [0.53965373, 0.86019145],\n",
       "       [0.52146648, 0.46505138],\n",
       "       [0.74682693, 0.47792556],\n",
       "       [0.66811362, 0.35466938],\n",
       "       [0.70386964, 0.56995215],\n",
       "       [0.61734981, 0.89902992],\n",
       "       [0.37891199, 0.79454838],\n",
       "       [0.65415489, 0.89032306],\n",
       "       [0.65380946, 0.71541457],\n",
       "       [0.51999472, 0.72699591],\n",
       "       [0.39045175, 0.72909892],\n",
       "       [0.36111348, 0.78077706],\n",
       "       [0.73869932, 0.48747369],\n",
       "       [0.69728716, 0.46328732],\n",
       "       [0.34668627, 0.82582   ],\n",
       "       [0.64777297, 0.61290894],\n",
       "       [0.60133641, 0.53626359],\n",
       "       [0.52410869, 0.90127555],\n",
       "       [0.6983093 , 0.42439306],\n",
       "       [0.51792054, 0.51746314],\n",
       "       [0.59873085, 0.61771667],\n",
       "       [0.45299597, 0.85006331],\n",
       "       [0.69256185, 0.33017688],\n",
       "       [0.50650177, 0.43646986],\n",
       "       [0.62969976, 0.43977246],\n",
       "       [0.7328519 , 0.47013387],\n",
       "       [0.54548229, 0.83143908],\n",
       "       [0.51790855, 0.84777082],\n",
       "       [0.46634128, 0.66412008],\n",
       "       [0.51385359, 0.86141724],\n",
       "       [0.73053091, 0.46855659],\n",
       "       [0.51703528, 0.86275226],\n",
       "       [0.60114922, 0.89179958],\n",
       "       [0.38392635, 0.7107444 ],\n",
       "       [0.62374333, 0.78410099],\n",
       "       [0.48688993, 0.81887766],\n",
       "       [0.67244675, 0.32885228],\n",
       "       [0.70488231, 0.44060894],\n",
       "       [0.50655081, 0.40012414],\n",
       "       [0.6230337 , 0.86763787],\n",
       "       [0.57650478, 0.38600335],\n",
       "       [0.75197089, 0.40824407],\n",
       "       [0.57510922, 0.50283828],\n",
       "       [0.54228894, 0.86050736],\n",
       "       [0.52351587, 0.34411104],\n",
       "       [0.47402942, 0.45425935],\n",
       "       [0.59536823, 0.43003453],\n",
       "       [0.54884956, 0.75248808],\n",
       "       [0.31442302, 0.66570898],\n",
       "       [0.57438881, 0.64036523],\n",
       "       [0.3906139 , 0.83375487],\n",
       "       [0.41345947, 0.80295691],\n",
       "       [0.50654737, 0.44269254],\n",
       "       [0.73907222, 0.73049441],\n",
       "       [0.63675193, 0.74803614],\n",
       "       [0.72637088, 0.53307568],\n",
       "       [0.42752994, 0.8163647 ],\n",
       "       [0.48297246, 0.67850068],\n",
       "       [0.60197448, 0.66602188],\n",
       "       [0.30536492, 0.77875709],\n",
       "       [0.7123436 , 0.45553813],\n",
       "       [0.5216094 , 0.74816587]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding the bias to the hidden layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = np.hstack((output_layer_1, np.ones((output_layer_1.shape[0],1))))\n",
    "h1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights for L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_2 = np.dot(h1, weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input to layer 2 for each sample after the dot product \n",
      "between features and weights is done but before activation:\n",
      "\n",
      "[[0.57759748]\n",
      " [0.48178422]\n",
      " [0.49519261]\n",
      " [0.45245709]\n",
      " [0.50918955]\n",
      " [0.5073215 ]\n",
      " [0.571062  ]\n",
      " [0.43851512]\n",
      " [0.41607657]\n",
      " [0.52720142]\n",
      " [0.55100725]\n",
      " [0.41418034]\n",
      " [0.42504474]\n",
      " [0.57357281]\n",
      " [0.43338378]\n",
      " [0.41909711]\n",
      " [0.52840854]\n",
      " [0.45133812]\n",
      " [0.58943316]\n",
      " [0.54749993]\n",
      " [0.5004337 ]\n",
      " [0.4875013 ]\n",
      " [0.44815838]\n",
      " [0.56925525]\n",
      " [0.54930393]\n",
      " [0.55704555]\n",
      " [0.56968329]\n",
      " [0.55559282]\n",
      " [0.5085557 ]\n",
      " [0.49546956]\n",
      " [0.46737214]\n",
      " [0.47972374]\n",
      " [0.51651874]\n",
      " [0.58254993]\n",
      " [0.46770516]\n",
      " [0.52739647]\n",
      " [0.48491248]\n",
      " [0.48479137]\n",
      " [0.42090847]\n",
      " [0.57815992]\n",
      " [0.56552944]\n",
      " [0.43988759]\n",
      " [0.51376023]\n",
      " [0.4874777 ]\n",
      " [0.50042956]\n",
      " [0.44865395]\n",
      " [0.49408948]\n",
      " [0.48849698]\n",
      " [0.46894025]\n",
      " [0.56141645]\n",
      " [0.50767704]\n",
      " [0.42522525]\n",
      " [0.47593424]\n",
      " [0.48989218]\n",
      " [0.50985625]\n",
      " [0.58440389]\n",
      " [0.47476565]\n",
      " [0.57890032]\n",
      " [0.4130251 ]\n",
      " [0.56143586]\n",
      " [0.41865406]\n",
      " [0.56324003]\n",
      " [0.50755892]\n",
      " [0.46757689]\n",
      " [0.51362271]\n",
      " [0.52550203]\n",
      " [0.52597524]\n",
      " [0.41406333]\n",
      " [0.57010484]\n",
      " [0.48289603]\n",
      " [0.54710345]\n",
      " [0.49524974]\n",
      " [0.44878491]\n",
      " [0.52066714]\n",
      " [0.5239812 ]\n",
      " [0.53251666]\n",
      " [0.48873323]\n",
      " [0.47070101]\n",
      " [0.53370922]\n",
      " [0.52077941]\n",
      " [0.47181088]\n",
      " [0.56462491]\n",
      " [0.54608472]\n",
      " [0.44574839]\n",
      " [0.42406552]\n",
      " [0.57074456]\n",
      " [0.44247277]\n",
      " [0.50507142]\n",
      " [0.43168084]\n",
      " [0.54144411]\n",
      " [0.49284169]\n",
      " [0.47965098]\n",
      " [0.52209735]\n",
      " [0.52513432]\n",
      " [0.49149559]\n",
      " [0.41593886]\n",
      " [0.44446555]\n",
      " [0.47853598]\n",
      " [0.50801745]\n",
      " [0.51452986]\n",
      " [0.47074061]\n",
      " [0.49252639]\n",
      " [0.482946  ]\n",
      " [0.44266701]\n",
      " [0.51823341]\n",
      " [0.46995207]\n",
      " [0.50627828]\n",
      " [0.55568914]\n",
      " [0.51915814]\n",
      " [0.46468196]\n",
      " [0.43088297]\n",
      " [0.45272767]\n",
      " [0.44830081]\n",
      " [0.47767595]\n",
      " [0.5666252 ]\n",
      " [0.50911939]\n",
      " [0.54336417]\n",
      " [0.52127799]\n",
      " [0.44453824]\n",
      " [0.50695239]\n",
      " [0.51947198]\n",
      " [0.51988874]\n",
      " [0.42374358]\n",
      " [0.45811159]\n",
      " [0.4912319 ]\n",
      " [0.49936976]\n",
      " [0.44414173]\n",
      " [0.4527529 ]\n",
      " [0.55764651]\n",
      " [0.53694692]\n",
      " [0.42986775]\n",
      " [0.52276365]\n",
      " [0.56721283]\n",
      " [0.44524862]\n",
      " [0.52045574]\n",
      " [0.46956948]\n",
      " [0.46923809]\n",
      " [0.54929309]\n",
      " [0.54069834]\n",
      " [0.48214601]\n",
      " [0.41359883]\n",
      " [0.41822947]\n",
      " [0.44202839]\n",
      " [0.5226595 ]\n",
      " [0.5804097 ]\n",
      " [0.5097069 ]\n",
      " [0.48137353]\n",
      " [0.52520182]\n",
      " [0.56614984]\n",
      " [0.58374942]\n",
      " [0.41769909]\n",
      " [0.42674707]\n",
      " [0.5955961 ]\n",
      " [0.46659739]\n",
      " [0.46869001]\n",
      " [0.55225167]\n",
      " [0.42010213]\n",
      " [0.49178012]\n",
      " [0.48275173]\n",
      " [0.56621483]\n",
      " [0.40658333]\n",
      " [0.48218919]\n",
      " [0.44410904]\n",
      " [0.41671239]\n",
      " [0.53419596]\n",
      " [0.5454948 ]\n",
      " [0.53179567]\n",
      " [0.54898488]\n",
      " [0.41718344]\n",
      " [0.54820464]\n",
      " [0.52656196]\n",
      " [0.56521066]\n",
      " [0.50196696]\n",
      " [0.5505195 ]\n",
      " [0.41267314]\n",
      " [0.42067862]\n",
      " [0.47626368]\n",
      " [0.51577322]\n",
      " [0.45204001]\n",
      " [0.4006556 ]\n",
      " [0.47147584]\n",
      " [0.53992369]\n",
      " [0.46183766]\n",
      " [0.49526058]\n",
      " [0.453287  ]\n",
      " [0.52030234]\n",
      " [0.57967376]\n",
      " [0.49406474]\n",
      " [0.58311699]\n",
      " [0.57094788]\n",
      " [0.48318676]\n",
      " [0.4570995 ]\n",
      " [0.49202487]\n",
      " [0.4289788 ]\n",
      " [0.56871763]\n",
      " [0.52892093]\n",
      " [0.48958984]\n",
      " [0.60089569]\n",
      " [0.42076745]\n",
      " [0.52813811]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "\n",
    "Input to layer 2 for each sample after the dot product \n",
    "between features and weights is done but before activation:\\n\n",
    "{input_layer_2}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation in L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_2 = sigmoid(input_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6405144 ],\n",
       "       [0.6181691 ],\n",
       "       [0.62132891],\n",
       "       [0.61122327],\n",
       "       [0.62461647],\n",
       "       [0.62417836],\n",
       "       [0.63900819],\n",
       "       [0.60790516],\n",
       "       [0.60254403],\n",
       "       [0.62883015],\n",
       "       [0.63436925],\n",
       "       [0.60208982],\n",
       "       [0.60468978],\n",
       "       [0.63958717],\n",
       "       [0.60668139],\n",
       "       [0.60326718],\n",
       "       [0.62911185],\n",
       "       [0.61095734],\n",
       "       [0.64323507],\n",
       "       [0.63355536],\n",
       "       [0.62256125],\n",
       "       [0.61951763],\n",
       "       [0.61020128],\n",
       "       [0.63859131],\n",
       "       [0.63397408],\n",
       "       [0.63576866],\n",
       "       [0.63869009],\n",
       "       [0.63543219],\n",
       "       [0.62446784],\n",
       "       [0.62139407],\n",
       "       [0.61476159],\n",
       "       [0.61768264],\n",
       "       [0.62633337],\n",
       "       [0.64165393],\n",
       "       [0.61484046],\n",
       "       [0.62887568],\n",
       "       [0.61890721],\n",
       "       [0.61887865],\n",
       "       [0.60370062],\n",
       "       [0.64064389],\n",
       "       [0.63773098],\n",
       "       [0.60823225],\n",
       "       [0.62568755],\n",
       "       [0.61951206],\n",
       "       [0.62256027],\n",
       "       [0.61031915],\n",
       "       [0.62106933],\n",
       "       [0.6197523 ],\n",
       "       [0.6151329 ],\n",
       "       [0.63678022],\n",
       "       [0.62426176],\n",
       "       [0.60473293],\n",
       "       [0.61678735],\n",
       "       [0.62008103],\n",
       "       [0.62477278],\n",
       "       [0.64208011],\n",
       "       [0.6165111 ],\n",
       "       [0.64081433],\n",
       "       [0.60181302],\n",
       "       [0.6367847 ],\n",
       "       [0.60316113],\n",
       "       [0.63720189],\n",
       "       [0.62423405],\n",
       "       [0.61481008],\n",
       "       [0.62565534],\n",
       "       [0.62843342],\n",
       "       [0.62854391],\n",
       "       [0.60206179],\n",
       "       [0.63878737],\n",
       "       [0.6184315 ],\n",
       "       [0.63346331],\n",
       "       [0.62134235],\n",
       "       [0.6103503 ],\n",
       "       [0.62730375],\n",
       "       [0.62807823],\n",
       "       [0.63006989],\n",
       "       [0.61980797],\n",
       "       [0.61554966],\n",
       "       [0.63034781],\n",
       "       [0.62733   ],\n",
       "       [0.61581228],\n",
       "       [0.63752198],\n",
       "       [0.63322674],\n",
       "       [0.6096279 ],\n",
       "       [0.60445568],\n",
       "       [0.63893496],\n",
       "       [0.60884809],\n",
       "       [0.62365039],\n",
       "       [0.60627497],\n",
       "       [0.63214829],\n",
       "       [0.62077563],\n",
       "       [0.61766545],\n",
       "       [0.62763807],\n",
       "       [0.62834756],\n",
       "       [0.62045869],\n",
       "       [0.60251105],\n",
       "       [0.60932257],\n",
       "       [0.61740211],\n",
       "       [0.6243416 ],\n",
       "       [0.62586778],\n",
       "       [0.61555903],\n",
       "       [0.6207014 ],\n",
       "       [0.61844329],\n",
       "       [0.60889434],\n",
       "       [0.62673459],\n",
       "       [0.61537241],\n",
       "       [0.62393361],\n",
       "       [0.6354545 ],\n",
       "       [0.62695089],\n",
       "       [0.61412428],\n",
       "       [0.60608449],\n",
       "       [0.61128756],\n",
       "       [0.61023516],\n",
       "       [0.61719893],\n",
       "       [0.63798409],\n",
       "       [0.62460002],\n",
       "       [0.63259466],\n",
       "       [0.62744655],\n",
       "       [0.60933987],\n",
       "       [0.62409177],\n",
       "       [0.62702429],\n",
       "       [0.62712175],\n",
       "       [0.60437871],\n",
       "       [0.6125661 ],\n",
       "       [0.62039659],\n",
       "       [0.62231121],\n",
       "       [0.60924548],\n",
       "       [0.61129356],\n",
       "       [0.63590781],\n",
       "       [0.63110191],\n",
       "       [0.60584209],\n",
       "       [0.62779377],\n",
       "       [0.6381198 ],\n",
       "       [0.60950896],\n",
       "       [0.62725433],\n",
       "       [0.61528185],\n",
       "       [0.61520341],\n",
       "       [0.63397157],\n",
       "       [0.63197485],\n",
       "       [0.6182545 ],\n",
       "       [0.60195049],\n",
       "       [0.6030595 ],\n",
       "       [0.60874225],\n",
       "       [0.62776943],\n",
       "       [0.64116167],\n",
       "       [0.62473776],\n",
       "       [0.61807216],\n",
       "       [0.62836332],\n",
       "       [0.6378743 ],\n",
       "       [0.64192969],\n",
       "       [0.60293253],\n",
       "       [0.60509663],\n",
       "       [0.64464812],\n",
       "       [0.61457809],\n",
       "       [0.61507365],\n",
       "       [0.63465784],\n",
       "       [0.60350769],\n",
       "       [0.62052569],\n",
       "       [0.61839744],\n",
       "       [0.63788931],\n",
       "       [0.60026834],\n",
       "       [0.61826469],\n",
       "       [0.6092377 ],\n",
       "       [0.60269629],\n",
       "       [0.63046122],\n",
       "       [0.63308972],\n",
       "       [0.62990182],\n",
       "       [0.63390004],\n",
       "       [0.60280908],\n",
       "       [0.63371895],\n",
       "       [0.62868089],\n",
       "       [0.63765733],\n",
       "       [0.62292146],\n",
       "       [0.63425611],\n",
       "       [0.60172867],\n",
       "       [0.60364563],\n",
       "       [0.61686521],\n",
       "       [0.62615887],\n",
       "       [0.61112416],\n",
       "       [0.59884517],\n",
       "       [0.61573301],\n",
       "       [0.63179467],\n",
       "       [0.61345003],\n",
       "       [0.6213449 ],\n",
       "       [0.61142046],\n",
       "       [0.62721846],\n",
       "       [0.64099234],\n",
       "       [0.62106351],\n",
       "       [0.64178431],\n",
       "       [0.63898187],\n",
       "       [0.6185001 ],\n",
       "       [0.61232587],\n",
       "       [0.62058332],\n",
       "       [0.60562979],\n",
       "       [0.63846722],\n",
       "       [0.6292314 ],\n",
       "       [0.6200098 ],\n",
       "       [0.6458612 ],\n",
       "       [0.60366688],\n",
       "       [0.62904875]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first prediction of the network but the results are not great yet, we just tried it with random weights. Let's now optimise, looking for the weights that give the best prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Feed-Forward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting column 2 on axis 1 (column) to transform X to the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.delete(X, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_sigmoid(X, weights0, weights1):\n",
    "    \n",
    "   \n",
    "    print(f\"Shape of the first X that you provided: {X.shape}\")\n",
    "    \n",
    "    \"\"\"\n",
    "    Add a bias column, so X turns from 50,2 into 50,3. \n",
    "    Doing this here so that it is clean every time we run the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    if X.shape == (200,2):\n",
    "        X_input = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "    else:\n",
    "        X_input = X\n",
    "        \n",
    "    print(f\"Shape of the first input with biases: {X_input.shape}\")\n",
    "\n",
    "\n",
    "    # dot product of X (200,3) with weights0(3,2) -- > (200,2).. \n",
    "    input_layer_1 = np.dot(X_input, weights0)\n",
    "    print(f\"Shape of the first input for layer 1 after dot product: {input_layer_1.shape}\")\n",
    "\n",
    "    # activation on l1 (shape still 200,2)\n",
    "    output_layer_1 = sigmoid(input_layer_1)\n",
    "    print(f\"Shape of the output of layer 1: {output_layer_1.shape}\")\n",
    "\n",
    "    # next layer's input with bias\n",
    "    hidden_1 = np.hstack((output_layer_1,np.ones((output_layer_1.shape[0], 1))))\n",
    "    print(f\"Shape of the hidden l1 input with biases: {output_layer_1.shape}\")\n",
    "\n",
    "    # after dot product: hidden_1(200,3) * weights1(3,1) --> 200,1\n",
    "    input_layer_2 = np.dot(hidden_1, weights1)\n",
    "    print(f\"\"\"Shape of the hidden l1 (overall l2) input weighted sum: {input_layer_2.shape}\"\"\")\n",
    "\n",
    "    #final activation\n",
    "    output_layer_2 = sigmoid(input_layer_2)\n",
    "    \n",
    "    return output_layer_1, output_layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "output_layer_1, output_layer_2 = feed_forward_sigmoid(X,weights1,weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to compare the predictions which will be a vector with one column, let's reshape the original y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrue.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Function and Calculate Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, ypred):\n",
    "    \n",
    "    \"\"\"log is natural logarithm\"\"\"\n",
    "    \n",
    "    return -(y * np.log(ypred) + (1-y) * np.log(1-ypred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also immediately define the derivative of this function as we will be using it later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_deriv(y, ypred):\n",
    "    \n",
    "    \"\"\"Derivative of log_loss with respect to ypred. \"\"\"\n",
    "   \n",
    "    return -(y/ypred-(1-y)/(1-ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148.0593283929185"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(ytrue, output_layer_2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember, until now we have done no training, we just plugged in some random initial weights!** We would expect this model to be terrible :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'badly predicted classes with random weights before training')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure(figsize=(15,6))\n",
    "ax1 = f.add_subplot(121)  # row 1, col 2, index 1\n",
    "ax2 = f.add_subplot(122)\n",
    "x = np.linspace(0,4,1000)\n",
    "ax1.scatter(X[:,0], X[:,1], c = y)\n",
    "ax1.set_title('real classes')\n",
    "\n",
    "ax2.scatter(X[:,0], X[:,1], c = output_layer_2)\n",
    "ax2.set_title('badly predicted classes with random weights before training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step: Backpropagation \n",
    "\n",
    "\n",
    "In backpropagation we calculate the partial derivative of the cost function with respect to every weight and bias value. \n",
    "\n",
    "We then use **gradient descent** to change the value of each parameter step by step. Our goal is to minimise the cost function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main take-away messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Backpropagation works with the same principle as gradient descent. You can think of neural networks as a lot of interconnected logistic regressions (if using sigmoid activation).\n",
    "* If something isn't working, try debugging by **checking the dimensions of your vectors**!\n",
    "* **Log loss** is a good loss function for binary classification because it has a high value for a poor prediction and low value for a good prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small aside: The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for \n",
    "$$F(x) = f(g(x))$$\n",
    "\n",
    "The derivative of $F$ with respect to $x$ is given by\n",
    "$$ \\dfrac{dF}{dx} = \\dfrac{df}{dg} \\cdot \\dfrac{dg}{dx} $$\n",
    "\n",
    "Equivalent statement with different notation:\n",
    "\n",
    "$$ F'(x) = f'(g(x)) \\cdot g'(x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is basically a slightly more complex application of Gradient descent but for multiple parameters at once with multiple implementations of the Chain Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps of Backpropagation in our Network:"
   ]
  },
  {
   "attachments": {
    "ffn_backprop_steps.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ffn_backprop_steps.png](attachment:ffn_backprop_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![backprop_equations.png](backprop_equations.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above we have: \n",
    "- $\\cdot$ is a **dot product**\n",
    "- $\\times$ is **regular multiplication**\n",
    "- $\\nabla$ is a **gradient**\n",
    "- $\\Delta$ is a **delta** or **change in**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Log Loss work? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{loss} = -(y_{true} \\log(y_{pred}) + (1-y_{true}) \\log(1-y_{pred})) $$\n",
    "\n",
    "In the below image you can see visually what happens ($a_i$ corresponds to $y_{pred}$ in our example)"
   ]
  },
  {
   "attachments": {
    "log_loss_single_datapoint.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![log_loss_single_datapoint.png](attachment:log_loss_single_datapoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[source](https://dasha.ai/en-us/blog/log-loss-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worked example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1:**  $y_{true} = 1$\n",
    "\n",
    "\n",
    "$$ \\text{loss} = -\\log(y_{pred}) $$\n",
    "\n",
    "\\\n",
    "Try plugging in some predicted values for y:    \n",
    "\n",
    "$$y_{pred} = 0.001 ;\\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\text{loss} = -\\log(0.001) = 6.9$$ \n",
    "\n",
    "- This prediction is very bad so the loss is high\n",
    "\n",
    "$$y_{pred} = 0.999 ;\\, \\, \\, \\, \\, \\, \\, \\,  \\, \\, \\, \\, \\text{loss} = -\\log(0.999) = 0.001$$ \n",
    "- This prediction is pretty good, so the loss is low\n",
    "\n",
    "\n",
    "So the $-\\log(y_{pred})$ gives reasonable loss values for the ytrue = 1 case.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Case 2:**  $y_{true} = 0$\n",
    "\n",
    "$$ \\text{loss} = -\\log(1-y_{pred})$$\n",
    "\n",
    "\\,\n",
    "\n",
    "Similarly to before:\n",
    "$$y_{pred} = 0.999 ;\\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\,  \\text{loss} = -\\log(1-0.999) = 6.9$$\n",
    "\n",
    "$$y_{pred} = 0.001 ;\\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\, \\text{loss} = -\\log(1-0.001) = 0.001$$\n",
    "\n",
    "\n",
    "So the $-log(1-y_{pred})$ gives reasonable loss values for the ytrue = 0 case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation A. Take The Derivative of The Loss Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$error = Loss'(ytrue, ypred)$$\n",
    "\n",
    "The \"error\" is in fact the derivative of the error.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial O_{out}} = -(\\frac{y}{ypred}-\\frac{1-y}{1-ypred})$$\n",
    "\n",
    "Remember how in gradient descent we calculated the gradient of the MSE to minimise it step by step? Backpropagation works with the same principle but in this case we are using the log loss as the loss function.\n",
    "\n",
    "https://medium.com/analytics-vidhya/derivative-of-log-loss-function-for-logistic-regression-9b832f025c2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We defined a handy log loss derivative funtion above so we can just use it now\n",
    "deriv_log_loss = log_loss_deriv(ytrue, output_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deriv_log_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation B. Calculate The Derivative of The Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the activation function will contribute to the gradient of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\nabla y = act'(H_O \\cdot w_O) \\times error$$  \n",
    " \n",
    "\n",
    "\n",
    "It comes from the below usage of chain rule.\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial O_{in}} =\\frac{\\partial L}{\\partial O_{out}} \\cdot \\frac{\\partial O_{out}}{\\partial O_{in}} $$\n",
    "\n",
    "\n",
    "\n",
    "Then the new part:\n",
    "\n",
    "$$\\frac{\\partial O_{out}}{\\partial O_{in}} = act'(O_{in}) = act'(H_O \\cdot w_O) $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the $H_O \\cdot w_O$ term above we need to add a bias to our output from the first layer and do a dot product with the outer weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(x):\n",
    "    \n",
    "    return np.hstack([x, np.ones((x.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_out_with_bias = add_bias(output_layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already defined a derivative of our activation function above, so we can just use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_deriv_1 = sigmoid_der(hidden_out_with_bias.dot(weights2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "ygrad_l2 =  sig_deriv_1 * deriv_log_loss\n",
    "print(ygrad_l2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation C. Calculate Weight Change between 2 Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjustment in the weights is proportional to the negative of the loss gradient. Recall: we multiply the negative of the loss gradient by some small constant that we call the **learning rate** to determine the step size.\n",
    "\n",
    "\n",
    "$$\\Delta{w_O} = -( \\frac{\\partial L}{\\partial O_{in}} \\cdot \\frac{\\partial O_{in}}{\\partial w_O}) \\times LR $$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\Delta w_O = -(\\nabla y \\cdot H_{out}) \\times LR$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$$H_{out}=\\frac{\\partial O_{in}}{\\partial w_O}$$\n",
    "\n",
    "The updated weights are and the next weight at this layer will be:\n",
    "\n",
    "$$ weights_{O_{new}} = weights_O + \\Delta w_O$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjustment in the weights is proportional to the negative of the loss gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose some value for the learning rate:\n",
    "lr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 1), (200, 3))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ygrad_l2.shape, hidden_out_with_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the change in weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_w2 = -np.dot(ygrad_l2.transpose(), \n",
    "                   hidden_out_with_bias) * lr\n",
    "delta_w2.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.02998606, -5.87860258, -4.87476824]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.31345624],\n",
       "       [ 0.16260876],\n",
       "       [ 0.56998151]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape makes sense as we have 3 weights to update for the one neuron. However, the shape of `weights2` is `(3,1)` so to add them we need to transpose our `delta_w2` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2_new = weights2 + delta_w2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.3434423 ],\n",
       "       [-5.71599382],\n",
       "       [-4.30478673]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights2_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are repeating a similar process to step B for our first (hidden) layer with 2 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So, we can write the full equation as:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial H_{in}} = (\\frac{\\partial L}{\\partial O_{in}}\\cdot \\frac{\\partial O_{in}}{\\partial H_{out}}) \\times \\frac{\\partial H_{out}}{\\partial H_{in}} $$\n",
    "\n",
    ".\n",
    "\n",
    "where \n",
    "\n",
    "$$\\frac{\\partial H_{out}}{\\partial H_{in}} = act'(X \\cdot w_H) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial O_{in}}{\\partial H_{out}} = w_O$$\n",
    "\n",
    "\n",
    "Final equation becomes:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial H_{in}} = (\\frac{\\partial L}{\\partial O_{in}}  \\cdot w_O)\\times \\frac{\\partial H_{out}}{\\partial H_{in}} $$\n",
    "\n",
    "\n",
    "Or for our purposes:\n",
    "\n",
    "$$\\nabla H =  (\\nabla y \\cdot w_O) \\times act'(X \\cdot w_H)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_deriv_2 = sigmoid_der(add_bias(X).dot(weights1))\n",
    "\n",
    "# Remember, weights2 contains a bias term as well. \n",
    "# we don't pass this back to the previous layer so we take it out here:\n",
    "h_grad = sig_deriv_2 * np.dot(ygrad_l2 , weights2[:-1].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 1), (3, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ygrad_l2.shape, weights2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(h_grad.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation E. Calculate Change in the first Set of Weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating step C for the first layer with 2 neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjustment in the weights is proportional to the negative of the loss gradient.\n",
    "\n",
    "\n",
    "$$\\Delta w_H = -(\\nabla H \\cdot X) \\times LR$$\n",
    "\n",
    "where\n",
    "\n",
    "\n",
    "$$\\frac{\\partial H_{in}}{\\partial w_H} = X$$\n",
    "\n",
    "\n",
    "The change in weights are \n",
    "\n",
    "$$\\Delta_{w_1} = -( \\frac{\\partial L}{\\partial H_{in}} \\cdot X) \\times LR $$\n",
    "\n",
    "And the next weight at this layer will be:\n",
    "\n",
    "$$ weights_{1_{new}} = weights_1 + \\Delta w_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 2), (3, 2), (200, 2))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_grad.shape, weights1.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_w1 = -np.dot(h_grad.transpose(), add_bias(X)) * lr\n",
    "weights1_new = weights1 + delta_w1.transpose() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56415322, -0.23374001],\n",
       "       [ 0.35640848,  1.39480421],\n",
       "       [-0.1165223 ,  0.41516355]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02735711, 0.05241734],\n",
       "       [1.03984657, 1.17523753],\n",
       "       [0.26523634, 0.37462419]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your task is to write all of the above steps inside a function and use a for loop where the function is called with updated weights every time. \n",
    "\n",
    "See the [course material](https://spiced.space/regularised-aniseed/ds-course/chapters/project_deep_learning/neural_networks/backpropagation.html#step-5-write-a-backpropagation-function) for some hints to get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(output_layer_1, output_layer_2, lr, weights1, weights2):\n",
    "    \n",
    "    '''EQUATION A:'''\n",
    "    deriv_log_loss = log_loss_deriv(ytrue, output_layer_2)\n",
    "    print(f\"deriv_log_loss_shape = {deriv_log_loss.shape}\")\n",
    "\n",
    "    '''EQUATION B:'''\n",
    "    # don't forget the bias!\n",
    "    hidden_out_with_bias = add_bias(output_layer_1)\n",
    "    # derivative of the sigmoid function with respect to the\n",
    "    # hidden output * weights\n",
    "    sig_deriv_1 = sigmoid_der(hidden_out_with_bias.dot(weights2))\n",
    "\n",
    "    ygrad_l2 =  sig_deriv_1 * deriv_log_loss\n",
    "    print(f\"ygrad_l2_shape = {ygrad_l2.shape}\")\n",
    "    print(f\"hidden_out_with_bias_shape = {hidden_out_with_bias.shape}\")\n",
    "\n",
    "    '''EQUATION C:'''\n",
    "    delta_w2 = -np.dot(ygrad_l2.transpose(), \n",
    "                   hidden_out_with_bias) * lr\n",
    "    print(f\"delta_w2_shape = {delta_w2.shape}\")\n",
    "\n",
    "    #and finally, old weights + delta weights -> new weights!\n",
    "    weights2_new = weights2 + delta_w2.transpose()\n",
    "    \n",
    "\n",
    "    '''EQUATION D:'''\n",
    "    sig_deriv_2 = sigmoid_der(add_bias(X).dot(weights1))\n",
    "    #exclude the bias (last column) of the outer weights,\n",
    "    #since it is not backpropagated!\n",
    "    h_grad = sig_deriv_2 * np.dot(ygrad_l2 , weights2[:-1].T)\n",
    "    print(f\"ygrad_l2_shape = {ygrad_l2.shape}\")\n",
    "    print(f\"weights2_shape = {weights2.shape}\")\n",
    "    print(f\"h_grad_shape = {h_grad.shape}\")\n",
    "\n",
    "    '''EQUATION E:'''\n",
    "    print(f\"weights1_shape = {weights1.shape}\")\n",
    "    print(f\"X_shape = {X.shape}\")\n",
    "    delta_w1 = -np.dot(h_grad.transpose(), add_bias(X)) * lr\n",
    "\n",
    "    #old weights + delta weights -> new weights!\n",
    "    weights1_new = weights1 + delta_w1.transpose() \n",
    "    print(f\"delta_w1_shape = {delta_w1.shape}\")\n",
    "    \n",
    "    # new hidden weights, new output weights\n",
    "    \n",
    "    return weights1_new, weights2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n",
      "Shape of the first X that you provided: (200, 2)\n",
      "Shape of the first input with biases: (200, 3)\n",
      "Shape of the first input for layer 1 after dot product: (200, 2)\n",
      "Shape of the output of layer 1: (200, 2)\n",
      "Shape of the hidden l1 input with biases: (200, 2)\n",
      "Shape of the hidden l1 (overall l2) input weighted sum: (200, 1)\n",
      "deriv_log_loss_shape = (200, 1)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "hidden_out_with_bias_shape = (200, 3)\n",
      "delta_w2_shape = (1, 3)\n",
      "ygrad_l2_shape = (200, 1)\n",
      "weights2_shape = (3, 1)\n",
      "h_grad_shape = (200, 2)\n",
      "weights1_shape = (3, 2)\n",
      "X_shape = (200, 2)\n",
      "delta_w1_shape = (2, 3)\n"
     ]
    }
   ],
   "source": [
    "LOSS_VEC = []\n",
    "\n",
    "for i in range(1000):\n",
    "    output_layer_1, output_layer_2 = feed_forward_sigmoid(X, weights1, weights2)\n",
    "    LOSS_VEC.append(log_loss(ytrue, output_layer_2).sum())\n",
    "    w1_new, w2_new = backprop(output_layer_1, output_layer_2, lr, weights1, weights2)\n",
    "    weights1 = w1_new\n",
    "    weights2 = w2_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[148.0593283929185,\n",
       " 886.9424747707424,\n",
       " 1765.9910659404254,\n",
       " 436.0783889807222,\n",
       " 1500.8020521472436,\n",
       " 528.3500879911479,\n",
       " 1457.7816004989768,\n",
       " 549.4870222143024,\n",
       " 1432.9287110751645,\n",
       " 565.6994474536178,\n",
       " 1412.8716324580203,\n",
       " 579.2382405172526,\n",
       " 1389.0840589438108,\n",
       " 590.8420632872067,\n",
       " 1354.3779507107145,\n",
       " 600.9994607510782,\n",
       " 1311.129888641445,\n",
       " 609.8931162822371,\n",
       " 1235.2974460681392,\n",
       " 616.2884346966991,\n",
       " 972.2397220055584,\n",
       " 538.4420133427727,\n",
       " 834.2436989571306,\n",
       " 141.09824189709582,\n",
       " 372.69762870302804,\n",
       " 772.6103481950081,\n",
       " 245.13899447704952,\n",
       " 367.0322313150072,\n",
       " 705.7270762513718,\n",
       " 253.98579726402502,\n",
       " 153.60963801880953,\n",
       " 384.16215534636405,\n",
       " 120.4830237767806,\n",
       " 135.52091823492674,\n",
       " 323.15102474622773,\n",
       " 670.0209920988298,\n",
       " 178.96948751750318,\n",
       " 205.31512771658507,\n",
       " 517.4406827335192,\n",
       " 225.08472759890483,\n",
       " 195.22237200457874,\n",
       " 481.34821906221634,\n",
       " 203.90859775088524,\n",
       " 229.41797104075607,\n",
       " 529.5373979059827,\n",
       " 221.50572890089725,\n",
       " 156.1677606759801,\n",
       " 310.6793451519932,\n",
       " 113.25600531880255,\n",
       " 112.42966199509382,\n",
       " 163.71783234733044,\n",
       " 421.4579315257691,\n",
       " 140.5267083527487,\n",
       " 217.34218536823886,\n",
       " 553.6195760863707,\n",
       " 216.86097768695276,\n",
       " 433.434193256703,\n",
       " 182.49140047104936,\n",
       " 194.44152148218438,\n",
       " 403.8941708423053,\n",
       " 176.7150770556434,\n",
       " 129.05371857289606,\n",
       " 188.91391908439778,\n",
       " 142.7340915396129,\n",
       " 311.22700319545436,\n",
       " 126.37760050975291,\n",
       " 103.00124884086429,\n",
       " 157.3477660271715,\n",
       " 116.11943319879254,\n",
       " 255.33691234585868,\n",
       " 100.97446443080216,\n",
       " 97.46628716986275,\n",
       " 163.98835911288035,\n",
       " 114.13893727028307,\n",
       " 244.8802946212598,\n",
       " 92.79276026189595,\n",
       " 100.039485491123,\n",
       " 192.36487370258402,\n",
       " 87.26077576573714,\n",
       " 115.95338140851715,\n",
       " 161.93145123584924,\n",
       " 443.23283296483964,\n",
       " 265.0791538851388,\n",
       " 382.5738631914047,\n",
       " 222.58910939246334,\n",
       " 135.6698389556806,\n",
       " 108.42448222647067,\n",
       " 168.6939699648695,\n",
       " 122.00151837730968,\n",
       " 246.86577813914914,\n",
       " 97.14925923334397,\n",
       " 116.43972114775151,\n",
       " 255.58632085845292,\n",
       " 102.45943074302775,\n",
       " 102.3118840901871,\n",
       " 187.54648275944618,\n",
       " 95.8409706036496,\n",
       " 156.10022575773752,\n",
       " 117.72285790251715,\n",
       " 263.824198150251,\n",
       " 102.50148968973522,\n",
       " 109.02386466928635,\n",
       " 213.5755734269298,\n",
       " 85.02378903909232,\n",
       " 80.78011055609122,\n",
       " 80.51250933073905,\n",
       " 87.18972988207017,\n",
       " 141.6625987776419,\n",
       " 140.24874843018043,\n",
       " 364.1261262443825,\n",
       " 125.60706597028366,\n",
       " 107.37092890382212,\n",
       " 121.45085204226643,\n",
       " 259.8336852533917,\n",
       " 101.66064542338538,\n",
       " 125.42982548594432,\n",
       " 283.4422735283775,\n",
       " 109.587097538173,\n",
       " 115.57079464362243,\n",
       " 229.08746962835426,\n",
       " 89.74418440246657,\n",
       " 100.53015332450656,\n",
       " 199.2186705224516,\n",
       " 83.46320202635164,\n",
       " 94.84058396129085,\n",
       " 156.06978544819978,\n",
       " 446.43046233252727,\n",
       " 363.4474368831915,\n",
       " 446.11631939115307,\n",
       " 261.4419523628024,\n",
       " 171.72235666112073,\n",
       " 107.03439595107807,\n",
       " 90.60226886041164,\n",
       " 84.25674718236768,\n",
       " 85.7927845255074,\n",
       " 104.00266486065465,\n",
       " 219.48372663871785,\n",
       " 98.50194207655272,\n",
       " 128.875400076534,\n",
       " 151.9278786431139,\n",
       " 360.3692480509343,\n",
       " 160.09105284611942,\n",
       " 101.2209458060256,\n",
       " 92.58507932861686,\n",
       " 113.7366091621984,\n",
       " 258.6190000492383,\n",
       " 95.458117083818,\n",
       " 107.52814539014221,\n",
       " 219.5023332568884,\n",
       " 88.8831094391483,\n",
       " 82.00443310111473,\n",
       " 80.54125471674914,\n",
       " 81.60209064382538,\n",
       " 96.92780207002822,\n",
       " 194.95302768143918,\n",
       " 88.48590065902148,\n",
       " 131.95243592777325,\n",
       " 144.1915650385202,\n",
       " 372.67580755676005,\n",
       " 128.5112457487604,\n",
       " 141.3695935455036,\n",
       " 110.32844390520515,\n",
       " 210.26892483561147,\n",
       " 85.39415623499758,\n",
       " 81.83415211661517,\n",
       " 81.19732724911657,\n",
       " 81.83629852067239,\n",
       " 91.42494054267696,\n",
       " 161.78207400225585,\n",
       " 114.84445576366159,\n",
       " 261.17152217746815,\n",
       " 97.40041430313411,\n",
       " 109.89553103137905,\n",
       " 223.02553504517692,\n",
       " 85.28901890542186,\n",
       " 93.16603402399943,\n",
       " 167.79784024076656,\n",
       " 100.6080648142629,\n",
       " 203.92309815809813,\n",
       " 82.32073005355855,\n",
       " 87.44295468711644,\n",
       " 134.3561297980351,\n",
       " 360.99525175449173,\n",
       " 172.57910845697484,\n",
       " 297.99956087100986,\n",
       " 151.88322153984353,\n",
       " 102.17019956187283,\n",
       " 114.19314120340302,\n",
       " 137.344213592217,\n",
       " 326.34181985914176,\n",
       " 126.35925185809015,\n",
       " 112.13817418055727,\n",
       " 210.83444414939447,\n",
       " 85.35993301011722,\n",
       " 85.45523318176122,\n",
       " 114.45528521673631,\n",
       " 165.01843186862837,\n",
       " 436.87137353452886,\n",
       " 172.60734855933526,\n",
       " 247.8530432631946,\n",
       " 126.02688063837599,\n",
       " 118.43860216264213,\n",
       " 229.73352540606962,\n",
       " 91.81259205798935,\n",
       " 89.87934940732683,\n",
       " 128.4770505578307,\n",
       " 162.15917341141858,\n",
       " 381.54847028781523,\n",
       " 172.21201525915262,\n",
       " 104.24045866591896,\n",
       " 89.44737066078518,\n",
       " 83.37006420110153,\n",
       " 90.94508848054801,\n",
       " 154.29165955012954,\n",
       " 134.19850025235382,\n",
       " 314.2911092362958,\n",
       " 125.64265116634965,\n",
       " 117.59799558638045,\n",
       " 222.43286648027498,\n",
       " 90.23346671209956,\n",
       " 105.0977647396902,\n",
       " 215.5041606360894,\n",
       " 83.08244904135297,\n",
       " 82.76703629999426,\n",
       " 101.39794117193453,\n",
       " 169.2581951130538,\n",
       " 471.9104281341969,\n",
       " 328.8923696886676,\n",
       " 436.0373181312592,\n",
       " 266.8278731658942,\n",
       " 173.02649260123928,\n",
       " 105.93540019591592,\n",
       " 89.34717428602534,\n",
       " 87.392821947454,\n",
       " 108.63627768017419,\n",
       " 189.69560182527607,\n",
       " 430.0197880127892,\n",
       " 205.2595269112671,\n",
       " 125.74459701563795,\n",
       " 107.76955543795927,\n",
       " 176.4414550106266,\n",
       " 94.47634940899417,\n",
       " 150.05071298686488,\n",
       " 125.24488285537146,\n",
       " 287.2377368268914,\n",
       " 117.41961388856981,\n",
       " 124.0961951459214,\n",
       " 249.54524571563002,\n",
       " 102.06119708869585,\n",
       " 125.95967279301365,\n",
       " 280.3753202374306,\n",
       " 113.03499753335522,\n",
       " 137.62776212987626,\n",
       " 310.28683367831576,\n",
       " 124.80154469302133,\n",
       " 121.37856493934723,\n",
       " 243.89475336922038,\n",
       " 98.78867031563126,\n",
       " 130.86922931079636,\n",
       " 301.8039281475503,\n",
       " 110.77449823803127,\n",
       " 127.76852128591327,\n",
       " 272.6140724839854,\n",
       " 109.63877800391643,\n",
       " 138.15296253842305,\n",
       " 318.5586171403927,\n",
       " 109.88721720911117,\n",
       " 124.81861340213786,\n",
       " 258.5064852988037,\n",
       " 105.81741488457192,\n",
       " 140.95233357099593,\n",
       " 329.00105848588623,\n",
       " 113.34533453114592,\n",
       " 122.60019363150616,\n",
       " 243.9877559568704,\n",
       " 100.0721997207093,\n",
       " 131.8849650623039,\n",
       " 302.64331519851555,\n",
       " 113.47413881447085,\n",
       " 128.28840551636335,\n",
       " 273.88716192296033,\n",
       " 110.95027323301349,\n",
       " 138.6591581826853,\n",
       " 318.9280795018836,\n",
       " 111.38221539772015,\n",
       " 126.47527095891782,\n",
       " 263.87499614946535,\n",
       " 108.91622206417043,\n",
       " 141.75373292240724,\n",
       " 328.8333215645014,\n",
       " 114.593830554921,\n",
       " 124.57354547258723,\n",
       " 250.944853926529,\n",
       " 103.92541743948145,\n",
       " 137.07775285594943,\n",
       " 315.9451836037324,\n",
       " 116.80624359267233,\n",
       " 126.41562188044333,\n",
       " 261.0961683121077,\n",
       " 108.01468999816977,\n",
       " 139.34437641253584,\n",
       " 322.00545667397165,\n",
       " 112.96634080297873,\n",
       " 126.601486038871,\n",
       " 262.12880581210044,\n",
       " 108.91504918629276,\n",
       " 141.5882167873442,\n",
       " 327.5172630991519,\n",
       " 115.82095911204163,\n",
       " 125.79707075574247,\n",
       " 255.48990613371814,\n",
       " 106.48156390018154,\n",
       " 139.03134181571252,\n",
       " 320.16499946237803,\n",
       " 117.39947092318835,\n",
       " 126.54005467350379,\n",
       " 260.73808706476933,\n",
       " 108.4442285422686,\n",
       " 140.2724245569371,\n",
       " 323.73996060009466,\n",
       " 114.96790238707976,\n",
       " 127.04064275583255,\n",
       " 262.48038695443256,\n",
       " 109.66215638247465,\n",
       " 141.5301191989563,\n",
       " 326.25148418491835,\n",
       " 117.01338438358157,\n",
       " 126.56918705352496,\n",
       " 259.11312399166513,\n",
       " 108.39074616895545,\n",
       " 140.65355947513774,\n",
       " 323.8072052968488,\n",
       " 117.84724725469087,\n",
       " 126.6703132298756,\n",
       " 260.4219473062677,\n",
       " 108.81927423889067,\n",
       " 141.00010421545846,\n",
       " 324.8935978177831,\n",
       " 116.94261662677405,\n",
       " 127.08618603829116,\n",
       " 262.01978913524243,\n",
       " 109.78255498507488,\n",
       " 141.66770882142174,\n",
       " 326.0949216650984,\n",
       " 117.82584352434738,\n",
       " 126.8940320121767,\n",
       " 260.87243971281924,\n",
       " 109.37537488948087,\n",
       " 141.67780642691196,\n",
       " 326.05457280932603,\n",
       " 118.41409065365181,\n",
       " 126.7939771172483,\n",
       " 260.5282718676306,\n",
       " 109.22364353205286,\n",
       " 141.7227443302151,\n",
       " 326.1919229863097,\n",
       " 118.42391537074639,\n",
       " 126.92509299932291,\n",
       " 261.16008399697125,\n",
       " 109.5936468961838,\n",
       " 142.05471129530616,\n",
       " 326.86927298737305,\n",
       " 118.68395774788924,\n",
       " 126.92923766966794,\n",
       " 261.09206597643504,\n",
       " 109.65961772936774,\n",
       " 142.28725626532292,\n",
       " 327.3214428534062,\n",
       " 119.14551813085595,\n",
       " 126.82358809546544,\n",
       " 260.6341563317493,\n",
       " 109.48828951463722,\n",
       " 142.39772768838424,\n",
       " 327.5821736219249,\n",
       " 119.4159475359793,\n",
       " 126.79757205959574,\n",
       " 260.56810016170255,\n",
       " 109.50129842788567,\n",
       " 142.57028899155344,\n",
       " 327.96136924085636,\n",
       " 119.63141201048077,\n",
       " 126.79608418318222,\n",
       " 260.58368837023,\n",
       " 109.56841092324512,\n",
       " 142.78849146015716,\n",
       " 328.4238883259188,\n",
       " 119.93237617326827,\n",
       " 126.73918738494392,\n",
       " 260.3494645907351,\n",
       " 109.50736094107766,\n",
       " 142.95841456834611,\n",
       " 328.797251720211,\n",
       " 120.24456506329534,\n",
       " 126.66418498175472,\n",
       " 260.0720085043006,\n",
       " 109.40943007533946,\n",
       " 143.10459317474698,\n",
       " 329.1372687344076,\n",
       " 120.49637290597083,\n",
       " 126.6159418111105,\n",
       " 259.9156138653582,\n",
       " 109.37205752340881,\n",
       " 143.2732657469513,\n",
       " 329.51690307192933,\n",
       " 120.74591439772978,\n",
       " 126.56430151071055,\n",
       " 259.7414692382251,\n",
       " 109.3264961468423,\n",
       " 143.44688018778893,\n",
       " 329.9071379519444,\n",
       " 121.01880591358727,\n",
       " 126.49085728313445,\n",
       " 259.47859977242047,\n",
       " 109.23053272537861,\n",
       " 143.59768306090535,\n",
       " 330.2547255296238,\n",
       " 121.28534413537011,\n",
       " 126.41364943158516,\n",
       " 259.2179564318135,\n",
       " 109.1284682137175,\n",
       " 143.74284350685377,\n",
       " 330.59712339707977,\n",
       " 121.52675635369803,\n",
       " 126.34690511578489,\n",
       " 259.00149718887224,\n",
       " 109.0488961044548,\n",
       " 143.89550646216148,\n",
       " 330.9500497218412,\n",
       " 121.77157053518607,\n",
       " 126.27352599828494,\n",
       " 258.7613273448961,\n",
       " 108.95456892794132,\n",
       " 144.04500592894692,\n",
       " 331.29885360821834,\n",
       " 122.02137572830279,\n",
       " 126.19082767332421,\n",
       " 258.48646058193776,\n",
       " 108.83830181877792,\n",
       " 144.180895747247,\n",
       " 331.61990373978347,\n",
       " 122.26419437575404,\n",
       " 126.10743610937249,\n",
       " 258.2218568392574,\n",
       " 108.72259865765238,\n",
       " 144.3158872216975,\n",
       " 331.94345808331747,\n",
       " 122.49233377053821,\n",
       " 126.02999595260769,\n",
       " 257.9790639072184,\n",
       " 108.61836593890669,\n",
       " 144.45250091858313,\n",
       " 332.2647518793686,\n",
       " 122.72531963361428,\n",
       " 125.9454151253859,\n",
       " 257.71468796618177,\n",
       " 108.49918460387876,\n",
       " 144.58395139535412,\n",
       " 332.57898943474123,\n",
       " 122.95599377667193,\n",
       " 125.85767278723682,\n",
       " 257.43920201944576,\n",
       " 108.37162782627593,\n",
       " 144.70561790072045,\n",
       " 332.87042169486415,\n",
       " 123.18100281735397,\n",
       " 125.77013167380278,\n",
       " 257.1764314084256,\n",
       " 108.24696751655247,\n",
       " 144.82880901610332,\n",
       " 333.1694824538197,\n",
       " 123.39477377703676,\n",
       " 125.6877053720444,\n",
       " 256.9282578390383,\n",
       " 108.13087003452752,\n",
       " 144.95062691215696,\n",
       " 333.45818773541873,\n",
       " 123.61535846016692,\n",
       " 125.5971473367766,\n",
       " 256.658269351362,\n",
       " 107.9987876013075,\n",
       " 145.066308532623,\n",
       " 333.7395864194352,\n",
       " 123.82886968953655,\n",
       " 125.50799905028109,\n",
       " 256.3927249059549,\n",
       " 107.867963321679,\n",
       " 145.1747776871608,\n",
       " 334.0010161636767,\n",
       " 124.03806443605963,\n",
       " 125.41954232327637,\n",
       " 256.14139996630877,\n",
       " 107.74153328337445,\n",
       " 145.28698375300448,\n",
       " 334.27535349577727,\n",
       " 124.23798212695698,\n",
       " 125.33594349398366,\n",
       " 255.89976230783705,\n",
       " 107.6219466670102,\n",
       " 145.39532533848302,\n",
       " 334.53204100647724,\n",
       " 124.44717199574538,\n",
       " 125.24259383404797,\n",
       " 255.63363748017713,\n",
       " 107.48401588186455,\n",
       " 145.49604942746186,\n",
       " 334.78032917970563,\n",
       " 124.64462812895087,\n",
       " 125.15528123175022,\n",
       " 255.3878255452293,\n",
       " 107.35720702350696,\n",
       " 145.59354693523997,\n",
       " 335.01557061388485,\n",
       " 124.83823260745378,\n",
       " 125.06980960530056,\n",
       " 255.15793966238863,\n",
       " 107.23692139438107,\n",
       " 145.69690075792795,\n",
       " 335.2673239411366,\n",
       " 125.02705106588141,\n",
       " 124.98644798021232,\n",
       " 254.92536691440205,\n",
       " 107.11680197520326,\n",
       " 145.79200196863115,\n",
       " 335.4916179127699,\n",
       " 125.22613861434235,\n",
       " 124.89244711552783,\n",
       " 254.6683339233573,\n",
       " 106.97748495609733,\n",
       " 145.87764648435132,\n",
       " 335.7052622705802,\n",
       " 125.40712937834654,\n",
       " 124.81089640280905,\n",
       " 254.45516397118357,\n",
       " 106.86357915777312,\n",
       " 145.96924994160852,\n",
       " 335.92551477979003,\n",
       " 125.58454466973946,\n",
       " 124.7327076480831,\n",
       " 254.2549281173401,\n",
       " 106.75703165534404,\n",
       " 146.06669617014143,\n",
       " 336.15730695555254,\n",
       " 125.76934104664667,\n",
       " 124.64654767087552,\n",
       " 254.01811779359133,\n",
       " 106.62959129707329,\n",
       " 146.14376257900102,\n",
       " 336.3391884960266,\n",
       " 125.95863147845931,\n",
       " 124.553865674036,\n",
       " 253.77817290606066,\n",
       " 106.49387490452328,\n",
       " 146.21547714931884,\n",
       " 336.5220885307257,\n",
       " 126.11637034694849,\n",
       " 124.48757298944813,\n",
       " 253.62827195182734,\n",
       " 106.41304940549529,\n",
       " 146.31521920690383,\n",
       " 336.7565963528958,\n",
       " 126.28152216339447,\n",
       " 124.41709076505013,\n",
       " 253.44575003216954,\n",
       " 106.3172711872081,\n",
       " 146.40419993305403,\n",
       " 336.9561853710875,\n",
       " 126.4798037685109,\n",
       " 124.31400039857033,\n",
       " 253.15736237315156,\n",
       " 106.15314568059296,\n",
       " 146.44213400188517,\n",
       " 337.0543876452226,\n",
       " 126.64762837175493,\n",
       " 124.23411060447097,\n",
       " 252.98844738129128,\n",
       " 106.05050264473957,\n",
       " 146.52055964008161,\n",
       " 337.26041039012205,\n",
       " 126.75764281457357,\n",
       " 124.21045769729326,\n",
       " 252.97693685053002,\n",
       " 106.05553051982483,\n",
       " 146.66674100020924,\n",
       " 337.578051114397,\n",
       " 126.94956834701347,\n",
       " 124.11606470405606,\n",
       " 252.67308821575,\n",
       " 105.89351600685025,\n",
       " 146.6920464248492,\n",
       " 337.61225552312806,\n",
       " 127.19785446119509,\n",
       " 123.95863303628667,\n",
       " 252.24735508466748,\n",
       " 105.62693223173527,\n",
       " 146.63907941721106,\n",
       " 337.5468409261955,\n",
       " 127.25085345288636,\n",
       " 123.9796988838429,\n",
       " 252.49064915099808,\n",
       " 105.75838321675542,\n",
       " 146.88066935878498,\n",
       " 338.1348280064705,\n",
       " 127.27828970861312,\n",
       " 124.04363898363565,\n",
       " 252.64267958785564,\n",
       " 105.90401949843101,\n",
       " 147.10834158996784,\n",
       " 338.5200490890901,\n",
       " 127.73606807070607,\n",
       " 123.69981490815698,\n",
       " 251.4230603973125,\n",
       " 105.18713240915558,\n",
       " 146.6856581413523,\n",
       " 337.56090402080093,\n",
       " 127.97697008220311,\n",
       " 123.52387341643703,\n",
       " 251.28719293644542,\n",
       " 104.99504073226367,\n",
       " 146.691177608933,\n",
       " 337.8593168650623,\n",
       " 127.40048580977225,\n",
       " 124.15249059209907,\n",
       " 253.54114432638772,\n",
       " 106.40113597649335,\n",
       " 147.84518815163398,\n",
       " 340.29920079163037,\n",
       " 127.75140236206944,\n",
       " 123.95626471442297,\n",
       " 252.10896339447726,\n",
       " 105.75815879791843,\n",
       " 147.49702362760883,\n",
       " 339.00188836828465,\n",
       " 129.40514703936623,\n",
       " 122.40123756521595,\n",
       " 247.12723736548364,\n",
       " 102.61053457900275,\n",
       " 144.47674687450635,\n",
       " 332.4583247454293,\n",
       " 128.17357076606189,\n",
       " 123.77201862145303,\n",
       " 254.499407030512,\n",
       " 106.37790601379004,\n",
       " 148.32392920894736,\n",
       " 342.9245825704492,\n",
       " 124.33301007290015,\n",
       " 127.29876207078246,\n",
       " 263.1758879415099,\n",
       " 113.1320146198696,\n",
       " 149.6811853136811,\n",
       " 341.43065406400746,\n",
       " 132.77739371889402,\n",
       " 119.57649938970447,\n",
       " 236.71998250992942,\n",
       " 96.86856277757923,\n",
       " 134.01248632703425,\n",
       " 305.7090883813656,\n",
       " 124.25925050777103,\n",
       " 133.20804153145073,\n",
       " 291.0524050146479,\n",
       " 123.13055551963006,\n",
       " 139.8564665953558,\n",
       " 313.72175490748236,\n",
       " 124.02377605802397,\n",
       " 133.9765430612454,\n",
       " 292.01078033849313,\n",
       " 123.99318453744792,\n",
       " 139.0607699344747,\n",
       " 310.57674861410163,\n",
       " 123.93273458648298,\n",
       " 134.71180897804396,\n",
       " 294.6741873325822,\n",
       " 124.09044852285803,\n",
       " 138.30261687231658,\n",
       " 307.8967046351033,\n",
       " 123.60282036530332,\n",
       " 135.56322310682614,\n",
       " 297.68727207747634,\n",
       " 124.18072705807585,\n",
       " 137.53074223942903,\n",
       " 305.0641209035716,\n",
       " 123.5349999915242,\n",
       " 136.23694806319054,\n",
       " 300.1523500556615,\n",
       " 123.96999134227175,\n",
       " 137.07859901771667,\n",
       " 303.347628809143,\n",
       " 123.55343727721224,\n",
       " 136.60644812262785,\n",
       " 301.51992088830787,\n",
       " 123.79345548162213,\n",
       " 136.88874166022072,\n",
       " 302.59415131889466,\n",
       " 123.63884537157128,\n",
       " 136.75043165570509,\n",
       " 302.04520884807505,\n",
       " 123.73790519002978,\n",
       " 136.83306184655225,\n",
       " 302.3542726954022,\n",
       " 123.70373389911943,\n",
       " 136.80049039336905,\n",
       " 302.2153888030858,\n",
       " 123.74458028616067,\n",
       " 136.82523297917658,\n",
       " 302.3011001433222,\n",
       " 123.74746768864362,\n",
       " 136.8217549976247,\n",
       " 302.2760870888162,\n",
       " 123.7699303160664,\n",
       " 136.8315235363901,\n",
       " 302.3036077361145,\n",
       " 123.7827347035433,\n",
       " 136.83515087224998,\n",
       " 302.306809283985,\n",
       " 123.80008889508727,\n",
       " 136.84140928055066,\n",
       " 302.32067829479,\n",
       " 123.8152245735469,\n",
       " 136.84650807914636,\n",
       " 302.3299605103574,\n",
       " 123.83122949110798,\n",
       " 136.8520049719554,\n",
       " 302.3409833823147,\n",
       " 123.84674768166288,\n",
       " 136.85730414487625,\n",
       " 302.3512914276319,\n",
       " 123.86234365699255,\n",
       " 136.86261350474769,\n",
       " 302.3617564291177,\n",
       " 123.87779853383556,\n",
       " 136.86787191552298,\n",
       " 302.37211159992535,\n",
       " 123.89319078016166,\n",
       " 136.8730925244339,\n",
       " 302.3824182004968,\n",
       " 123.90849754369381,\n",
       " 136.87827626092908,\n",
       " 302.3926751380085,\n",
       " 123.9237247097555,\n",
       " 136.88342097113252,\n",
       " 302.4028741249084,\n",
       " 123.93887311769979,\n",
       " 136.88852881306843,\n",
       " 302.4130225889532,\n",
       " 123.95394258133966,\n",
       " 136.89359926740286,\n",
       " 302.4231175102916,\n",
       " 123.96893453296846,\n",
       " 136.89863303426915,\n",
       " 302.43316086036964,\n",
       " 123.98384935055915,\n",
       " 136.90363038117863,\n",
       " 302.44315276154987,\n",
       " 123.99868789755976,\n",
       " 136.90859166326564,\n",
       " 302.453093779251,\n",
       " 124.01345084829012,\n",
       " 136.9135172438398,\n",
       " 302.46298448444975,\n",
       " 124.0281389110538,\n",
       " 136.91840745421405,\n",
       " 302.47282534375154,\n",
       " 124.04275278809013,\n",
       " 136.92326263973928,\n",
       " 302.48261688800693,\n",
       " 124.05729316262322,\n",
       " 136.92808313320597,\n",
       " 302.4923596071885,\n",
       " 124.07176071700646,\n",
       " 136.9328692656801,\n",
       " 302.5020539958778,\n",
       " 124.08615612173028,\n",
       " 136.9376213639584,\n",
       " 302.5117005405746,\n",
       " 124.10048004031131,\n",
       " 136.94233975031312,\n",
       " 302.5212997203624,\n",
       " 124.11473312810787,\n",
       " 136.94702474335975,\n",
       " 302.530852008905,\n",
       " 124.12891603241017,\n",
       " 136.95167665756327,\n",
       " 302.54035787319975,\n",
       " 124.14302939286317,\n",
       " 136.95629580353892,\n",
       " 302.54981777430066,\n",
       " 124.15707384134761,\n",
       " 136.96088248804597,\n",
       " 302.55923216716866,\n",
       " 124.17105000220283,\n",
       " 136.96543701403556,\n",
       " 302.5686015007833,\n",
       " 124.18495849230354,\n",
       " 136.9699596807255,\n",
       " 302.57792621842725,\n",
       " 124.1987999211679,\n",
       " 136.974450783639,\n",
       " 302.58720675753887,\n",
       " 124.2125748910704,\n",
       " 136.97891061466686,\n",
       " 302.5964435499029,\n",
       " 124.22628399714195,\n",
       " 136.98333946211375,\n",
       " 302.6056370217066,\n",
       " 124.23992782747678,\n",
       " 136.98773761075103,\n",
       " 302.6147875937694,\n",
       " 124.25350696322799,\n",
       " 136.99210534187097,\n",
       " 302.623895681509,\n",
       " 124.26702197871259,\n",
       " 136.9964429333283,\n",
       " 302.6329616948683,\n",
       " 124.28047344150616,\n",
       " 137.00075065960021,\n",
       " 302.6419860387665,\n",
       " 124.29386191254176,\n",
       " 137.00502879182056,\n",
       " 302.65096911293074,\n",
       " 124.30718794620189,\n",
       " 137.0092775978434,\n",
       " 302.65991131197626,\n",
       " 124.32045209041333,\n",
       " 137.01349734227352,\n",
       " 302.66881302553975,\n",
       " 124.33365488674123,\n",
       " 137.0176882865257,\n",
       " 302.67767463836026,\n",
       " 124.34679687047438,\n",
       " 137.02185068886007,\n",
       " 302.6864965304364,\n",
       " 124.35987857071783,\n",
       " 137.02598480443527,\n",
       " 302.6952790768987,\n",
       " 124.37290051047985,\n",
       " 137.03009088534336,\n",
       " 302.7040226482313,\n",
       " 124.38586320675819,\n",
       " 137.03416918065938,\n",
       " 302.71272761033487,\n",
       " 124.398767170622,\n",
       " 137.03821993648393,\n",
       " 302.72139432455117,\n",
       " 124.41161290729971,\n",
       " 137.04224339597735,\n",
       " 302.7300231477791,\n",
       " 124.42440091625714,\n",
       " 137.04623979941357,\n",
       " 302.738614432462,\n",
       " 124.43713169128017,\n",
       " 137.05020938420586,\n",
       " 302.747168526732,\n",
       " 124.44980572055337,\n",
       " 137.05415238496073,\n",
       " 302.75568577449235,\n",
       " 124.46242348673732,\n",
       " 137.05806903350816,\n",
       " 302.764166515484,\n",
       " 124.47498546705013,\n",
       " 137.06195955893986,\n",
       " 302.772611085091,\n",
       " 124.48749213333548,\n",
       " 137.06582418765663,\n",
       " 302.781019814914,\n",
       " 124.49994395214246,\n",
       " 137.0696631433937,\n",
       " 302.7893930322638,\n",
       " 124.51234138479943,\n",
       " 137.0734766472662,\n",
       " 302.79773106073003,\n",
       " 124.52468488748048,\n",
       " 137.07726491780306,\n",
       " 302.80603421980993,\n",
       " 124.53697491128321,\n",
       " 137.08102817097978,\n",
       " 302.81430282534325,\n",
       " 124.54921190229484,\n",
       " 137.08476662025856,\n",
       " 302.82253718925404,\n",
       " 124.5613963016599,\n",
       " 137.08848047662005,\n",
       " 302.8307376197935,\n",
       " 124.57352854565191,\n",
       " 137.0921699485965,\n",
       " 302.83890442148123,\n",
       " 124.58560906573592,\n",
       " 137.09583524230902,\n",
       " 302.847037895403,\n",
       " 124.59763828863633,\n",
       " 137.0994765614966,\n",
       " 302.85513833881055,\n",
       " 124.6096166364019,\n",
       " 137.10309410755008,\n",
       " 302.86320604572416,\n",
       " 124.62154452646499,\n",
       " 137.1066880795486,\n",
       " 302.87124130648976,\n",
       " 124.63342237171128,\n",
       " 137.11025867427983,\n",
       " 302.8792444081315,\n",
       " 124.64525058053522,\n",
       " 137.11380608628585,\n",
       " 302.88721563428066,\n",
       " 124.65702955690142,\n",
       " 137.11733050788106,\n",
       " 302.89515526542255,\n",
       " 124.6687597004082,\n",
       " 137.12083212918975,\n",
       " 302.90306357852137,\n",
       " 124.68044140634095,\n",
       " 137.124311138173,\n",
       " 302.9109408475373,\n",
       " 124.69207506573434,\n",
       " 137.12776772065774,\n",
       " 302.9187873431314,\n",
       " 124.70366106542704,\n",
       " 137.13120206036677,\n",
       " 302.9266033328409,\n",
       " 124.71519978811703,\n",
       " 137.1346143389455,\n",
       " 302.93438908129923,\n",
       " 124.72669161241906,\n",
       " 137.13800473599167,\n",
       " 302.9421448499139,\n",
       " 124.73813691291585,\n",
       " 137.14137342908168,\n",
       " 302.94987089721565,\n",
       " 124.74953606021522,\n",
       " 137.14472059379617,\n",
       " 302.9575674786955,\n",
       " 124.76088942100006,\n",
       " 137.1480464037503,\n",
       " 302.96523484702993,\n",
       " 124.77219735807938,\n",
       " 137.15135103061675,\n",
       " 302.9728732519701,\n",
       " 124.78346023044274,\n",
       " 137.15463464415245,\n",
       " 302.98048294055764,\n",
       " 124.79467839330765,\n",
       " 137.15789741222494,\n",
       " 302.9880641567497,\n",
       " 124.80585219817,\n",
       " 137.16113950083587,\n",
       " 302.99561714226616,\n",
       " 124.81698199285213,\n",
       " 137.16436107414654,\n",
       " 303.0031421355518,\n",
       " 124.82806812155117,\n",
       " 137.16756229450343,\n",
       " 303.01063937280435,\n",
       " 124.83911092488702,\n",
       " 137.17074332245866,\n",
       " 303.0181090874396,\n",
       " 124.85011073994897,\n",
       " 137.17390431679536,\n",
       " 303.0255515102965,\n",
       " 124.8610679003377,\n",
       " 137.17704543455534,\n",
       " 303.03296686961056,\n",
       " 124.87198273621624,\n",
       " 137.18016683105134,\n",
       " 303.04035539119525,\n",
       " 124.88285557435113,\n",
       " 137.18326865990116,\n",
       " 303.04771729815604,\n",
       " 124.89368673815447,\n",
       " 137.18635107304203,\n",
       " 303.05505281151113,\n",
       " 124.90447654773156,\n",
       " 137.18941422075588,\n",
       " 303.06236214952145,\n",
       " 124.9152253199204,\n",
       " 137.19245825168778,\n",
       " 303.06964552820347,\n",
       " 124.92593336833225,\n",
       " 137.19548331287353,\n",
       " 303.0769031612987,\n",
       " 124.93660100339513,\n",
       " 137.1984895497526,\n",
       " 303.0841352600137,\n",
       " 124.94722853239671,\n",
       " 137.20147710619239,\n",
       " 303.0913420335347,\n",
       " 124.95781625951702,\n",
       " 137.20444612451118,\n",
       " 303.0985236885467,\n",
       " 124.96836448587399,\n",
       " 137.20739674549264,\n",
       " 303.10568042965957,\n",
       " 124.97887350956097,\n",
       " 137.21032910840876,\n",
       " 303.1128124592017,\n",
       " 124.9893436256828,\n",
       " 137.21324335103952,\n",
       " 303.11991997743974,\n",
       " 124.99977512639518,\n",
       " 137.2161396096916,\n",
       " 303.1270031823233,\n",
       " 125.0101683009441,\n",
       " 137.21901801921067]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOSS_VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9258efd880>]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(LOSS_VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some cool links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: [But how can we use ReLU in Backpropagation if its derivative is not defined at zero?](https://www.quora.com/Why-does-ReLU-work-with-backprops-if-its-non-differentiable)\n",
    "- [Josh Starmer's excellent Neural Networks playlist](https://youtube.com/playlist?list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1) which includes a great intuitive explanation of backpropagation\n",
    "- [Nice visual explanation of the backpropagation process](https://developers-dot-devsite-v2-prod.appspot.com/machine-learning/crash-course/backprop-scroll)\n",
    "- [3Blue1Brown on Backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Agenda",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
